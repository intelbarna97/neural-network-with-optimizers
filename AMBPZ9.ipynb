{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyONnjXRreDbN1S86i9jKdco",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/intelbarna97/neural-network-with-optimizers/blob/main/AMBPZ9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4KCx4tJEkG8Y"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "import math, random, time\n",
        "\n",
        "from sklearn.metrics import mean_squared_error"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DataTransform:\n",
        "  def __init__(self, row_start, row_end, history_length, source, delimiter=\",\"):\n",
        "    self.data = []\n",
        "    self.start = row_start\n",
        "    self.end = row_end\n",
        "    self.history_length = history_length\n",
        "    self.source = source\n",
        "    self.delimiter=delimiter\n",
        "\n",
        "  def is_number(self, s):\n",
        "    try:\n",
        "        float(s)\n",
        "        return True\n",
        "    except ValueError:\n",
        "        return False\n",
        "\n",
        "  def reader(self):\n",
        "    self.data = [sor.strip().split(sep=self.delimiter) for sor in open(self.source)]\n",
        "\n",
        "  def process(self):\n",
        "\n",
        "    self.reader()\n",
        "    self.data=[[float(adat) if self.is_number(adat) else adat  for adat in sor[self.start-1:self.end] ] for sor in self.data[1:]]\n",
        "\n",
        "    split = self.split()\n",
        "    reshape = self.reshape(split[0], split[2])\n",
        "\n",
        "    return reshape[0], reshape[1], split[1], split[3]\n",
        "\n",
        "  def split(self):\n",
        "    train = self.data[:round(len(self.data)*0.75)]\n",
        "    test = self.data[round(len(self.data)*0.75):]\n",
        "    extreme = self.extreme()\n",
        "    min, max = extreme[0], extreme[1]\n",
        "\n",
        "\n",
        "    train_x = (np.array([ train[i:i+self.history_length] for i in range(len(train)-self.history_length)])+min)/(min+max)\n",
        "    train_y = (np.array([ train[i+self.history_length] for i in range(len(train)-self.history_length)])+min)/(min+max)\n",
        "    test_x = (np.array([ test[i:i+self.history_length] for i in range(len(test)-self.history_length)])+min)/(min+max)\n",
        "    test_y = (np.array([ test[i+self.history_length] for i in range(len(test)-self.history_length)])+min)/(min+max)\n",
        "\n",
        "    return train_x, train_y, test_x, test_y\n",
        "\n",
        "  def extreme(self):\n",
        "    data = np.array(self.data)\n",
        "    return min(data.flatten())<0 if abs(min(data.flatten())) else 0, abs(max(data.flatten()))+1\n",
        "\n",
        "  def reshape(self, train, test):\n",
        "    reshaped_train_x = train.reshape(train.shape[0],train.shape[1]*train.shape[2])\n",
        "    reshaped_test_x = test.reshape(test.shape[0],test.shape[1]*test.shape[2])\n",
        "    return reshaped_train_x, reshaped_test_x\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kzMVIOWI0yIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Optimizers:\n",
        "\n",
        "  def __init__(self, weight_list, optimizer_name, learning_rate):\n",
        "    self.weight_list = weight_list\n",
        "    self.optimizer_name = optimizer_name\n",
        "    self.ml=[np.zeros_like(i) for i in weight_list]\n",
        "    self.vl=[np.zeros_like(i) for i in weight_list]\n",
        "    self.learning_rate = learning_rate\n",
        "\n",
        "  def adam(self, grad, l, epoch, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=0.00000001):\n",
        "    self.ml[l] = beta1 * self.ml[l] + (1-beta1)*grad\n",
        "    self.vl[l] = beta2 * self.vl[l] + (1-beta2)*np.power(grad,2)\n",
        "    mh = self.ml[l]/(1-np.power(beta1,epoch+1))\n",
        "    vh = self.vl[l]/(1-np.power(beta2,epoch+1))\n",
        "    self.weight_list[l] += learning_rate*mh/(np.sqrt(vh)+epsilon)\n",
        "    return self.weight_list\n",
        "\n",
        "  def nadam(self, grad, l, epoch, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=0.00000001):\n",
        "    self.ml[l] = beta1 * self.ml[l] + (1-beta1)*grad\n",
        "    self.vl[l] = beta2 * self.vl[l] + (1-beta2)*np.power(grad,2)\n",
        "    mh = self.ml[l]/(1-np.power(beta1,epoch+1)) + (1 - beta1)*grad/(1 - np.power(beta1,epoch+1))\n",
        "    vh = self.vl[l]/(1-np.power(beta2,epoch+1))\n",
        "    self.weight_list[l] += learning_rate*mh/(np.sqrt(vh)+epsilon)\n",
        "    return self.weight_list\n",
        "\n",
        "  def simple(self, l, grad, learning_rate=0.03):\n",
        "    self.weight_list[l] += learning_rate * grad\n",
        "    return self.weight_list\n",
        "\n",
        "  def momentum(self, l, grad, learning_rate=0.03):\n",
        "    self.weight_list[l] += 0.1*self.ml[l]\n",
        "    self.ml[l][:]=learning_rate * grad\n",
        "    self.weight_list[l] += 0.9*self.ml[l]\n",
        "    return self.weight_list\n",
        "\n",
        "\n",
        "  def execute(self, l, delta, neurons, neuronlayer_list, epoch):\n",
        "\n",
        "    grad = delta[l].reshape((neurons[l+1],1))*neuronlayer_list[l].reshape((1,neurons[l]))\n",
        "\n",
        "    if(self.optimizer_name == \"adam\"):\n",
        "\n",
        "      return self.adam(grad, l, epoch, self.learning_rate)\n",
        "\n",
        "    elif(self.optimizer_name == \"nadam\"):\n",
        "\n",
        "      return self.nadam(grad, l, epoch, self.learning_rate)\n",
        "\n",
        "    elif(self.optimizer_name == \"simple\"):\n",
        "\n",
        "      return self.simple(l, grad, self.learning_rate)\n",
        "\n",
        "    elif(self.optimizer_name == \"momentum\"):\n",
        "\n",
        "      return self.momentum(l, grad, self.learning_rate)\n",
        "\n",
        "    else:\n",
        "\n",
        "      return self.simple(l, grad, self.learning_rate)\n"
      ],
      "metadata": {
        "id": "-jWs6rDukMFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class mlp:\n",
        "\n",
        "  def activation_tanh(self, x):     return np.tanh(x)\n",
        "  def dactivation_tanh(self, x):    return 1.0 - x**2\n",
        "  def activation_GCU(self, x):      return x*np.cos(x)\n",
        "  def dactivation_GCU(self, x):     return np.cos(x)-x*np.sin(x)\n",
        "  def activation_sigmoid(self, x):  return 1.0/(1.0 + np.exp(-x))\n",
        "  def dactivation_sigmoid(self, x): return x*(1.0-x)\n",
        "\n",
        "\n",
        "  def __init__(self, input, output, test_input, test_output, acti_name, layer1, layer2, learning_rate, optimizer_name):\n",
        "    self.input = input\n",
        "    self.output = output\n",
        "    self.bias = 1\n",
        "    self.sumerr = len(input)\n",
        "    self.input_test = test_input\n",
        "    self.output_test = test_output\n",
        "\n",
        "\n",
        "    if(acti_name == \"tanh\"):\n",
        "      self.acti = self.activation_tanh\n",
        "      self.dacti = self.dactivation_tanh\n",
        "    elif(acti_name == \"GCU\"):\n",
        "      self.acti = self.activation_GCU\n",
        "      self.dacti = self.dactivation_GCU\n",
        "    elif(acti_name == \"sigmoid\"):\n",
        "      self.acti = self.activation_GCU\n",
        "      self.dacti = self.dactivation_GCU\n",
        "    else:\n",
        "      self.acti = self.activation_tanh\n",
        "      self.dacti = self.dactivation_tanh\n",
        "\n",
        "    self.neurons=[len(self.input[0])+self.bias, layer1, layer2, len(self.output[0])]\n",
        "\n",
        "    self.weighted_list = [np.random.random((self.neurons[l+1], self.neurons[l]))*0.4-0.2 for l in range(len(self.neurons)-1)]\n",
        "\n",
        "    self.delta = [np.zeros((self.neurons[l+1])) for l in range(len(self.neurons)-1)]\n",
        "\n",
        "    self.optimizer = Optimizers(self.weighted_list,optimizer_name, learning_rate)\n",
        "\n",
        "\n",
        "  def backward_propagation(self, output, neuronlayer_list, epoch):\n",
        "    error = output - neuronlayer_list[-1]\n",
        "\n",
        "    for l in reversed(range(len(self.neurons)-1)):\n",
        "            if l == len(self.neurons)-2:\n",
        "                self.delta[l][:] = error*self.dacti(neuronlayer_list[-1])\n",
        "            else:\n",
        "                np.dot(self.delta[l+1],self.weighted_list[l+1], out=self.delta[l])\n",
        "                self.delta[l] *= self.dacti(neuronlayer_list[l+1])\n",
        "            self.weighted_list = self.optimizer.execute(l, self.delta, self.neurons, neuronlayer_list, epoch)\n",
        "    return error\n",
        "\n",
        "\n",
        "  def testing(self):\n",
        "    sumerr = 0.0\n",
        "    compare=[]\n",
        "    for inp, out in zip(self.input_test, self.output_test):\n",
        "        nl = [ np.array(list(inp) + [1.0]*self.bias) ]\n",
        "        for l in range(len(self.neurons)-1):\n",
        "            nl.append(self.acti(np.dot(self.weighted_list[l],nl[l])))\n",
        "        compare.append(nl[-1])\n",
        "    compare=np.array(compare)\n",
        "    print('mean squared error :', mean_squared_error(self.output_test, compare))\n",
        "\n",
        "\n",
        "\n",
        "  def fit(self, epoch_range=20):\n",
        "\n",
        "    if(epoch_range<1):\n",
        "      epoch_range=20\n",
        "    epoch = 0\n",
        "\n",
        "    time1=time.time()\n",
        "    asd=[]\n",
        "    while(self.sumerr/len(self.input)>=0.00001 and epoch<epoch_range):\n",
        "      self.sumerr=0.0\n",
        "      epoch+=1\n",
        "      time2=time.time()\n",
        "\n",
        "      for input, output in zip(self.input, self.output):\n",
        "        neuronlayer_list=[np.array(list(input) + [1.0]*self.bias)]\n",
        "\n",
        "        for l in range(len(self.neurons)-1):\n",
        "          neuronlayer_list.append(self.acti(np.dot(self.weighted_list[l],neuronlayer_list[l])))\n",
        "\n",
        "        backward = self.backward_propagation(output, neuronlayer_list, epoch)\n",
        "        error = backward\n",
        "        self.sumerr += sum( [error[j]**2 for j in range(self.neurons[-1])])\n",
        "      print (\"iter #\"+str(epoch),self.sumerr/len(self.input), round(time.time()-time2,3))\n",
        "      break\n",
        "    print (\"Summary: \",self.sumerr/len(self.input), round(time.time()-time1,3))\n",
        "    self.testing()\n",
        "\n"
      ],
      "metadata": {
        "id": "tAFx_01DkO6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = DataTransform(2, 6, 3, \"/content/AAPL.csv\")\n",
        "processed_data = data.process()\n",
        "print(processed_data[0].shape,\n",
        "processed_data[1].shape,\n",
        "processed_data[2].shape,\n",
        "processed_data[3].shape,\n",
        "len(processed_data[0]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ElQim0RQOZn",
        "outputId": "b239097f-c356-4d6f-94e7-02bee420bea9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8061, 15) (2685, 15) (8061, 5) (2685, 5) 8061\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "test = mlp(processed_data[0], processed_data[2], processed_data[1], processed_data[3], \"tanh\", 10, 10, 0.003, \"adam\")\n",
        "\n",
        "test.fit(20)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPnYNtAokQrp",
        "outputId": "f51bcece-69ca-4307-cde5-3686ff5bbe44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter #1 4.2736187373303426e-06 1.374\n",
            "Summary:  4.2736187373303426e-06 1.376\n",
            "mean squared error : 0.07824879291172444\n"
          ]
        }
      ]
    }
  ]
}